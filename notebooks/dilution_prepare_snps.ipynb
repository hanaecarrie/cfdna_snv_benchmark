{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pysam import FastaFile\n",
    "from tqdm.notebook import tqdm \n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from supporting_reads import list_reads_to_remove, prepare_bamsurgeon_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SNP databases\n",
    "\n",
    "## dbSNP\n",
    "\n",
    "common variants : ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/00-common_all.vcf\n",
    "\n",
    "nb_lines = 501220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def read_vcf(path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = [l for l in f if not l.startswith('##')]\n",
    "        res = None\n",
    "        for bi in tqdm(range(int(np.ceil(len(lines)/1000)))):\n",
    "            res_batch = pd.read_csv(io.StringIO(''.join([lines[0]] + lines[bi*1000 + 1:(bi+1)*1000 + 1])),\n",
    "                dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n",
    "                       'QUAL': str, 'FILTER': str, 'INFO': str}, sep='\\t')\n",
    "            if res is None:\n",
    "                if not res_batch[res_batch['#CHROM'] == '22'].empty:\n",
    "                    res = res_batch[res_batch['#CHROM'] == '22']\n",
    "            else:\n",
    "                if not res_batch[res_batch['#CHROM'] == '22'].empty:\n",
    "                    res = pd.concat([res, res_batch[res_batch['#CHROM'] == '22']])\n",
    "        return res\n",
    "\n",
    "foo_type = lambda x: pd.Series(x.split(';VC=')[1].split(';')[0])\n",
    "\n",
    "dbsnp_df = read_vcf('../data/common_SNPs/00-common_all.vcf')\n",
    "print(dbsnp_df.shape)\n",
    "dbsnp_df.to_csv('../data/common_SNPs/dbsnp_df.csv', index=False)\n",
    "dbsnp_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsnp_df = pd.read_csv('../data/common_SNPs/dbsnp_df.csv')\n",
    "print(dbsnp_df.shape)\n",
    "dbsnp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## genomAD database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genomad_df = read_vcf('../data/common_SNPs/gnomad.genomes.r2.1.1.sites.22.vcf')\n",
    "#print(genomad_df.shape)\n",
    "#genomad_df.head()\n",
    "#genomad_df.to_csv('../data/common_SNPs/genomad_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator as the database is too large, even for chr22 only\n",
    "genomad_df_iterator = pd.read_csv('../data/common_SNPs/genomad_df.csv', iterator=True, chunksize=500000)\n",
    "genomad_df_iterator.get_chunk(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read patient SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_date = '986_100215'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def read_vcf(path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = [l for l in f if not l.startswith('##')]\n",
    "        res = pd.read_csv(io.StringIO(''.join(lines[:])),\n",
    "            dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n",
    "                   'QUAL': str, 'FILTER': str, 'INFO': str}, sep='\\t')\n",
    "        return res\n",
    "\n",
    "foo_vaf = lambda x: pd.Series(x.split(';AF=')[1].split(';')[0])\n",
    "\n",
    "# Read SNPs detected in cancer patient\n",
    "\n",
    "patient_snps_df = read_vcf('../data/2015-07-31_NCC_CRC-'+patient_date+'-CW/NCC_CRC-'+patient_date+'-CW-gatk-haplotype-annotated.vcf')\n",
    "print(patient_snps_df.shape)\n",
    "patient_snps_df = patient_snps_df[patient_snps_df['#CHROM'] == '22']\n",
    "print(patient_snps_df.shape)\n",
    "\n",
    "patient_snps_df['VAF'] = patient_snps_df['INFO'].apply(foo_vaf)\n",
    "patient_snps_df = patient_snps_df[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'VAF']]\n",
    "\n",
    "patient_snps_df.to_csv('../data/patient_SNPs/patient_'+patient_date.split('_')[0]+'_snps.csv', index=False)\n",
    "# print types of VAF\n",
    "# heterozygote (=0.5), homozygote (1), double hetoerozygotes (0.5,0.5)\n",
    "patient_snps_df['VAF'].value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_snps_df = pd.read_csv('../data/patient_SNPs/patient_'+patient_date.split('_')[0]+'_snps.csv')\n",
    "patient_snps_df['VAF'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find reads to remove in pooled healthy sample\n",
    "\n",
    "removing rare reads supporting known SNPs that are not found in the cancer patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important notes:\n",
    "1. Indexing\n",
    "    - SAM is a 1-index based file\n",
    "    - VCF is a 1-index based file\n",
    "    - pysam is a 0-index based tool\n",
    "\n",
    "2. Paired-End sequencing\n",
    "    - BAM/SAM are storing the resverse complementary of reversed reads as sequence\n",
    "\n",
    "3. Mapping issues\n",
    "    - some reads are not mapped -> no CIGAR string + no read.reference_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbSNP database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads2remove, log_pd = list_reads_to_remove(\n",
    "    \"../data/healthy_chr22_merged-ready.bam\", dbsnp_df.iloc[1000:1300],\n",
    "    patient_snps_df, verbose = -1)\n",
    "# save list of reads to remove and log dataframe\n",
    "#log_pd.to_csv('../data/prepare_pooled_healthy/log_'+patient_date.split('_')[0]+'_dbsnp.csv', index=False)\n",
    "#with open('../data/prepare_pooled_healthy/readfile_'+patient_date.split('_')[0]+'_dbsnp.txt', \"w\") as output:\n",
    "#    for r in reads2remove:\n",
    "#        output.write(str(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back results\n",
    "\n",
    "log_pd = pd.read_csv('../data/prepare_pooled_healthy/log_'+patient_date.split('_')[0]+'_dbsnp.csv')\n",
    "reads2remove = pd.read_csv('../data/prepare_pooled_healthy/readfile_'+patient_date.split('_')[0]+'_dbsnp.txt', header=None).values.flatten()\n",
    "print('# reads to remove: ', len(reads2remove))\n",
    "print('% reads to remove: {:2f}%'.format(100*sum(log_pd['supporting_reads'])/sum(log_pd['total_reads'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise results\n",
    "\n",
    "print('proportion of high VAF positions (above 10%): {}%'.format(100*log_pd[log_pd['vaf'] > 0.1].size/log_pd.size))\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('SNV')\n",
    "sns.histplot(data=log_pd[log_pd['type'] == 'SNV'][['vaf', 'normal af', 'noisy af']],\n",
    "             bins=100,  stat=\"probability\", common_norm=False, legend=False)\n",
    "plt.xlabel('local VAF')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('DEL')\n",
    "sns.histplot(data=log_pd[log_pd['type'] == 'DEL'][['vaf', 'normal af', 'noisy af']],\n",
    "             bins=100,  stat=\"probability\", common_norm=False, legend=False)\n",
    "plt.xlabel('local VAF')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('INS')\n",
    "sns.histplot(data=log_pd[log_pd['type'] == 'INS'][['vaf', 'normal af', 'noisy af']],\n",
    "             bins=100,  stat=\"probability\", common_norm=False, legend=False)\n",
    "plt.xlabel('local VAF')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('all mutations')\n",
    "sns.histplot(data=log_pd[['vaf', 'normal af', 'noisy af']],\n",
    "             bins=100,  stat=\"probability\", common_norm=False)\n",
    "\n",
    "log_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenomAD database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 0\n",
    "for genomad_df_chunk in genomad_df_iterator:\n",
    "    ci += 1\n",
    "    print('chunk '+str(ci))\n",
    "    genomad_df_chunk = genomad_df_chunk.drop('Unnamed: 0', axis=1)\n",
    "    reads2remove, log_pd = list_reads_to_remove(\n",
    "        \"../data/healthy_chr22_merged-ready.bam\", genomad_df_chunk, patient_snps_df, verbose=-1)\n",
    "    # save list of reads to remove and log dataframe\n",
    "    log_pd.to_csv('../data/prepare_pooled_healthy/log_'+patient_date.split('_')[0]+'_genomad_'+str(ci)+'.csv', index=False)\n",
    "    with open('../data/prepare_pooled_healthy/readfile_'+patient_date.split('_')[0]+'_genomad_'+str(ci)+'.txt', \"w\") as output:\n",
    "        for r in reads2remove:\n",
    "                    output.write(str(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient's SNPs detected with GATK Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_set = set(list(patient_snps_df.ID)) & set(list(dbsnp_df.ID))\n",
    "print(100*len(common_set)/dbsnp_df.size, 100*len(common_set)/patient_snps_df.size, dbsnp_df.size)\n",
    "\n",
    "# how many mutations are not known SNPs?\n",
    "print('% of unknown SNPs: {:2f}%'.format(100*patient_snps_df[patient_snps_df['ID'] == '.'].shape[0]/patient_snps_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patient_snps_df.size)\n",
    "print(dbsnp_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_snv_pd, bamsurgeon_indel_pd = prepare_bamsurgeon_inputs(patient_snps_df, log_pd, max_vaf=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "bamsurgeon_snv_pd.to_csv('../data/prepare_pooled_healthy/varfile_snv_'+patient_date.split('_')[0]+'_dbsnp.bed',  sep='\\t', header=False, index=False)\n",
    "bamsurgeon_indel_pd.to_csv('../data/prepare_pooled_healthy/varfile_indel_'+patient_date.split('_')[0]+'_dbsnp.bed',  sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_snv_pd = pd.read_csv('../data/prepare_pooled_healthy/varfile_snv_'+patient_date.split('_')[0]+'_dbsnp.bed', sep='\\t', header=None)\n",
    "bamsurgeon_snv_pd.columns = ['chrom', 'pos_start', 'pos_end', 'vaf', 'alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_snv_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split varfiles to run BamSurgeon by chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_snv_pd = pd.read_csv('../data/prepare_pooled_healthy/varfile_snv_'+patient_date.split('_')[0]+'_dbsnp.bed', sep='\\t', header=None)\n",
    "bamsurgeon_snv_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "for chunk in range(int(np.ceil(bamsurgeon_snv_pd.shape[0]/chunk_size))):\n",
    "    bamsurgeon_snv_pd_chunk = bamsurgeon_snv_pd.iloc[chunk*chunk_size:(chunk+1)*chunk_size]\n",
    "    bamsurgeon_snv_pd_chunk.to_csv('../data/prepare_pooled_healthy/varfile_snv_'+patient_date.split('_')[0]+'_dbsnp_'+str(chunk)+'.bed',  sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_indel_pd = pd.read_csv('../data/prepare_pooled_healthy/varfile_indel_'+patient_date.split('_')[0]+'_dbsnp.bed', sep='\\t', header=None)\n",
    "bamsurgeon_indel_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "for chunk in range(int(np.ceil(bamsurgeon_indel_pd.shape[0]/chunk_size))):\n",
    "    bamsurgeon_indel_pd_chunk = bamsurgeon_indel_pd.iloc[chunk*chunk_size:(chunk+1)*chunk_size]\n",
    "    bamsurgeon_indel_pd_chunk.to_csv('../data/prepare_pooled_healthy/varfile_indel_'+patient_date.split('_')[0]+'_dbsnp_'+str(chunk)+'.bed',  sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check SNV indel overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_indel_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamsurgeon_snv_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
